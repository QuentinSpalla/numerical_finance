{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Résolution d'EDP par réseau de neurones dans le cadre HJB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous reprenons l'algorithme codé par les auteurs dans le cadre Allen-Cahn (partie 4.2 de l'article) pour l'adapter au cadre HJB (partie 4.3 de l'article).\n",
    "\n",
    "L'algorithme de l'article s'intéresse à la résolution de l'équation différentielle :\n",
    "\n",
    "$$\\frac{\\partial_u}{\\partial_t}(t,x)+ \\frac{1}{2}Trace\\Big(\\sigma(t,x)[\\sigma(t,x)]*(Hess_x u)(t,x)\\Big) + <\\mu(t,x),(\\nabla_x u)(t,x)> +f\\Big(t,x,u(t,x),[(\\nabla_x u)(t,x)]*\\sigma (t,x)\\Big) = 0$$\n",
    "\n",
    "En particulier, dans le cadre HJB :\n",
    "$$f(t,x,y,z) = -\\vert \\vert z \\vert \\vert ^2$$\n",
    "\n",
    "$$g(x) = ln(\\frac{1}{2}[1+\\vert \\vert x \\vert \\vert ^2] )$$\n",
    "\n",
    "$$\\sigma(t,x)w = \\sqrt{2}w$$\n",
    "\n",
    "$$\\mu (t,x) = 0$$\n",
    "\n",
    "La solution de l'EDP est alors  :\n",
    "\n",
    "$$\\frac{\\partial_u}{\\partial_t}(t,x)+ (\\Delta_x u)(t,x) = \\vert \\vert (\\nabla_x u)(t,x)\\vert \\vert ^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Programs\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin to solve HJB\n",
      "WARNING:tensorflow:From <ipython-input-1-348851b7ba6b>:102: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "step:     0, loss: 1.7901e+01, Y0: 3.5068e-01,runtime:   31\n",
      "step:   10,loss:1.6999e+01,  Y0: 4.5058e-01,runtime:  43\n",
      "step:   20,loss:1.6028e+01,  Y0: 5.4995e-01,runtime:  44\n",
      "step:   30,loss:1.4994e+01,  Y0: 6.4841e-01,runtime:  46\n",
      "step:   40,loss:1.3862e+01,  Y0: 7.4537e-01,runtime:  47\n",
      "step:   50,loss:1.2632e+01,  Y0: 8.4033e-01,runtime:  48\n",
      "step:   60,loss:1.1238e+01,  Y0: 9.3259e-01,runtime:  50\n",
      "step:   70,loss:9.7112e+00,  Y0: 1.0218e+00,runtime:  51\n",
      "step:   80,loss:8.1476e+00,  Y0: 1.1078e+00,runtime:  52\n",
      "step:   90,loss:6.5952e+00,  Y0: 1.1883e+00,runtime:  53\n",
      "step:  100,loss:5.1858e+00,  Y0: 1.2623e+00,runtime:  54\n",
      "step:  110,loss:4.0413e+00,  Y0: 1.3280e+00,runtime:  55\n",
      "step:  120,loss:3.2671e+00,  Y0: 1.3833e+00,runtime:  57\n",
      "step:  130,loss:2.8424e+00,  Y0: 1.4282e+00,runtime:  58\n",
      "step:  140,loss:2.6664e+00,  Y0: 1.4635e+00,runtime:  59\n",
      "step:  150,loss:2.6083e+00,  Y0: 1.4922e+00,runtime:  60\n",
      "step:  160,loss:2.5924e+00,  Y0: 1.5128e+00,runtime:  61\n",
      "step:  170,loss:2.5811e+00,  Y0: 1.5315e+00,runtime:  62\n",
      "step:  180,loss:2.5625e+00,  Y0: 1.5479e+00,runtime:  64\n",
      "step:  190,loss:2.5372e+00,  Y0: 1.5646e+00,runtime:  65\n",
      "step:  200,loss:2.5151e+00,  Y0: 1.5813e+00,runtime:  66\n",
      "step:  210,loss:2.4955e+00,  Y0: 1.6002e+00,runtime:  67\n",
      "step:  220,loss:2.4785e+00,  Y0: 1.6193e+00,runtime:  68\n",
      "step:  230,loss:2.4685e+00,  Y0: 1.6385e+00,runtime:  70\n",
      "step:  240,loss:2.4530e+00,  Y0: 1.6561e+00,runtime:  71\n",
      "step:  250,loss:2.4311e+00,  Y0: 1.6761e+00,runtime:  72\n",
      "step:  260,loss:2.4072e+00,  Y0: 1.6985e+00,runtime:  73\n",
      "step:  270,loss:2.3880e+00,  Y0: 1.7206e+00,runtime:  74\n",
      "step:  280,loss:2.3689e+00,  Y0: 1.7415e+00,runtime:  76\n",
      "step:  290,loss:2.3476e+00,  Y0: 1.7625e+00,runtime:  77\n",
      "step:  300,loss:2.3247e+00,  Y0: 1.7826e+00,runtime:  78\n",
      "step:  310,loss:2.3117e+00,  Y0: 1.8040e+00,runtime:  79\n",
      "step:  320,loss:2.2914e+00,  Y0: 1.8293e+00,runtime:  81\n",
      "step:  330,loss:2.2712e+00,  Y0: 1.8590e+00,runtime:  82\n",
      "step:  340,loss:2.2622e+00,  Y0: 1.8835e+00,runtime:  83\n",
      "step:  350,loss:2.2459e+00,  Y0: 1.9083e+00,runtime:  84\n",
      "step:  360,loss:2.2252e+00,  Y0: 1.9340e+00,runtime:  85\n",
      "step:  370,loss:2.1968e+00,  Y0: 1.9607e+00,runtime:  86\n",
      "step:  380,loss:2.1653e+00,  Y0: 1.9861e+00,runtime:  87\n",
      "step:  390,loss:2.1453e+00,  Y0: 2.0099e+00,runtime:  89\n",
      "step:  400,loss:2.1212e+00,  Y0: 2.0332e+00,runtime:  90\n",
      "step:  410,loss:2.0926e+00,  Y0: 2.0597e+00,runtime:  91\n",
      "step:  420,loss:2.0671e+00,  Y0: 2.0875e+00,runtime:  93\n",
      "step:  430,loss:2.0435e+00,  Y0: 2.1153e+00,runtime:  94\n",
      "step:  440,loss:2.0108e+00,  Y0: 2.1459e+00,runtime:  95\n",
      "step:  450,loss:1.9786e+00,  Y0: 2.1791e+00,runtime:  96\n",
      "step:  460,loss:1.9515e+00,  Y0: 2.2090e+00,runtime:  98\n",
      "step:  470,loss:1.9246e+00,  Y0: 2.2377e+00,runtime:  99\n",
      "step:  480,loss:1.8880e+00,  Y0: 2.2636e+00,runtime: 100\n",
      "step:  490,loss:1.8586e+00,  Y0: 2.2886e+00,runtime: 102\n",
      "step:  500,loss:1.8314e+00,  Y0: 2.3186e+00,runtime: 103\n",
      "step:  510,loss:1.8035e+00,  Y0: 2.3505e+00,runtime: 104\n",
      "step:  520,loss:1.7747e+00,  Y0: 2.3788e+00,runtime: 105\n",
      "step:  530,loss:1.7485e+00,  Y0: 2.4070e+00,runtime: 106\n",
      "step:  540,loss:1.7235e+00,  Y0: 2.4363e+00,runtime: 107\n",
      "step:  550,loss:1.7016e+00,  Y0: 2.4658e+00,runtime: 109\n",
      "step:  560,loss:1.6767e+00,  Y0: 2.4981e+00,runtime: 110\n",
      "step:  570,loss:1.6479e+00,  Y0: 2.5312e+00,runtime: 111\n",
      "step:  580,loss:1.6228e+00,  Y0: 2.5627e+00,runtime: 113\n",
      "step:  590,loss:1.5933e+00,  Y0: 2.5942e+00,runtime: 114\n",
      "step:  600,loss:1.5590e+00,  Y0: 2.6266e+00,runtime: 115\n",
      "step:  610,loss:1.5317e+00,  Y0: 2.6569e+00,runtime: 116\n",
      "step:  620,loss:1.5126e+00,  Y0: 2.6889e+00,runtime: 117\n",
      "step:  630,loss:1.4878e+00,  Y0: 2.7234e+00,runtime: 118\n",
      "step:  640,loss:1.4628e+00,  Y0: 2.7611e+00,runtime: 119\n",
      "step:  650,loss:1.4308e+00,  Y0: 2.7918e+00,runtime: 121\n",
      "step:  660,loss:1.4013e+00,  Y0: 2.8195e+00,runtime: 122\n",
      "step:  670,loss:1.3659e+00,  Y0: 2.8498e+00,runtime: 123\n",
      "step:  680,loss:1.3289e+00,  Y0: 2.8833e+00,runtime: 124\n",
      "step:  690,loss:1.2933e+00,  Y0: 2.9204e+00,runtime: 126\n",
      "step:  700,loss:1.2615e+00,  Y0: 2.9565e+00,runtime: 127\n",
      "step:  710,loss:1.2302e+00,  Y0: 2.9919e+00,runtime: 129\n",
      "step:  720,loss:1.2029e+00,  Y0: 3.0274e+00,runtime: 130\n",
      "step:  730,loss:1.1771e+00,  Y0: 3.0618e+00,runtime: 132\n",
      "step:  740,loss:1.1497e+00,  Y0: 3.0945e+00,runtime: 133\n",
      "step:  750,loss:1.1242e+00,  Y0: 3.1267e+00,runtime: 134\n",
      "step:  760,loss:1.0933e+00,  Y0: 3.1572e+00,runtime: 135\n",
      "step:  770,loss:1.0641e+00,  Y0: 3.1908e+00,runtime: 136\n",
      "step:  780,loss:1.0382e+00,  Y0: 3.2306e+00,runtime: 138\n",
      "step:  790,loss:1.0095e+00,  Y0: 3.2695e+00,runtime: 139\n",
      "step:  800,loss:9.8005e-01,  Y0: 3.3044e+00,runtime: 140\n",
      "step:  810,loss:9.5222e-01,  Y0: 3.3376e+00,runtime: 142\n",
      "step:  820,loss:9.2647e-01,  Y0: 3.3715e+00,runtime: 143\n",
      "step:  830,loss:8.9542e-01,  Y0: 3.4058e+00,runtime: 145\n",
      "step:  840,loss:8.6751e-01,  Y0: 3.4412e+00,runtime: 147\n",
      "step:  850,loss:8.3438e-01,  Y0: 3.4792e+00,runtime: 148\n",
      "step:  860,loss:8.0474e-01,  Y0: 3.5155e+00,runtime: 150\n",
      "step:  870,loss:7.7448e-01,  Y0: 3.5497e+00,runtime: 152\n",
      "step:  880,loss:7.4584e-01,  Y0: 3.5840e+00,runtime: 154\n",
      "step:  890,loss:7.1499e-01,  Y0: 3.6205e+00,runtime: 156\n",
      "step:  900,loss:6.8517e-01,  Y0: 3.6556e+00,runtime: 157\n",
      "step:  910,loss:6.5769e-01,  Y0: 3.6904e+00,runtime: 159\n",
      "step:  920,loss:6.2577e-01,  Y0: 3.7263e+00,runtime: 161\n",
      "step:  930,loss:5.9456e-01,  Y0: 3.7623e+00,runtime: 163\n",
      "step:  940,loss:5.6353e-01,  Y0: 3.7959e+00,runtime: 165\n",
      "step:  950,loss:5.3558e-01,  Y0: 3.8294e+00,runtime: 166\n",
      "step:  960,loss:5.0808e-01,  Y0: 3.8629e+00,runtime: 168\n",
      "step:  970,loss:4.7749e-01,  Y0: 3.8949e+00,runtime: 170\n",
      "step:  980,loss:4.4810e-01,  Y0: 3.9270e+00,runtime: 172\n",
      "step:  990,loss:4.2056e-01,  Y0: 3.9598e+00,runtime: 173\n",
      "step: 1000,loss:3.9179e-01,  Y0: 3.9905e+00,runtime: 175\n",
      "step: 1010,loss:3.6563e-01,  Y0: 4.0207e+00,runtime: 177\n",
      "step: 1020,loss:3.4199e-01,  Y0: 4.0515e+00,runtime: 179\n",
      "step: 1030,loss:3.1789e-01,  Y0: 4.0814e+00,runtime: 180\n",
      "step: 1040,loss:2.9374e-01,  Y0: 4.1102e+00,runtime: 182\n",
      "step: 1050,loss:2.7123e-01,  Y0: 4.1383e+00,runtime: 184\n",
      "step: 1060,loss:2.5009e-01,  Y0: 4.1655e+00,runtime: 185\n",
      "step: 1070,loss:2.3183e-01,  Y0: 4.1913e+00,runtime: 187\n",
      "step: 1080,loss:2.1425e-01,  Y0: 4.2164e+00,runtime: 189\n",
      "step: 1090,loss:1.9535e-01,  Y0: 4.2399e+00,runtime: 191\n",
      "step: 1100,loss:1.7671e-01,  Y0: 4.2624e+00,runtime: 192\n",
      "step: 1110,loss:1.5937e-01,  Y0: 4.2867e+00,runtime: 194\n",
      "step: 1120,loss:1.4386e-01,  Y0: 4.3097e+00,runtime: 196\n",
      "step: 1130,loss:1.2983e-01,  Y0: 4.3298e+00,runtime: 197\n",
      "step: 1140,loss:1.1681e-01,  Y0: 4.3480e+00,runtime: 199\n",
      "step: 1150,loss:1.0434e-01,  Y0: 4.3669e+00,runtime: 201\n",
      "step: 1160,loss:9.3901e-02,  Y0: 4.3859e+00,runtime: 203\n",
      "step: 1170,loss:8.4117e-02,  Y0: 4.4049e+00,runtime: 204\n",
      "step: 1180,loss:7.4645e-02,  Y0: 4.4228e+00,runtime: 206\n",
      "step: 1190,loss:6.6354e-02,  Y0: 4.4384e+00,runtime: 208\n",
      "step: 1200,loss:5.8881e-02,  Y0: 4.4543e+00,runtime: 210\n",
      "step: 1210,loss:5.2447e-02,  Y0: 4.4682e+00,runtime: 211\n",
      "step: 1220,loss:4.6931e-02,  Y0: 4.4809e+00,runtime: 213\n",
      "step: 1230,loss:4.2280e-02,  Y0: 4.4936e+00,runtime: 215\n",
      "step: 1240,loss:3.8772e-02,  Y0: 4.5052e+00,runtime: 216\n",
      "step: 1250,loss:3.5819e-02,  Y0: 4.5160e+00,runtime: 218\n",
      "step: 1260,loss:3.3512e-02,  Y0: 4.5248e+00,runtime: 220\n",
      "step: 1270,loss:3.1493e-02,  Y0: 4.5323e+00,runtime: 222\n",
      "step: 1280,loss:2.9760e-02,  Y0: 4.5392e+00,runtime: 224\n",
      "step: 1290,loss:2.8072e-02,  Y0: 4.5452e+00,runtime: 225\n",
      "step: 1300,loss:2.6555e-02,  Y0: 4.5511e+00,runtime: 227\n",
      "step: 1310,loss:2.5387e-02,  Y0: 4.5565e+00,runtime: 229\n",
      "step: 1320,loss:2.4555e-02,  Y0: 4.5607e+00,runtime: 231\n",
      "step: 1330,loss:2.4011e-02,  Y0: 4.5647e+00,runtime: 232\n",
      "step: 1340,loss:2.3510e-02,  Y0: 4.5686e+00,runtime: 234\n",
      "step: 1350,loss:2.3029e-02,  Y0: 4.5721e+00,runtime: 236\n",
      "step: 1360,loss:2.2715e-02,  Y0: 4.5753e+00,runtime: 238\n",
      "step: 1370,loss:2.2478e-02,  Y0: 4.5788e+00,runtime: 239\n",
      "step: 1380,loss:2.2356e-02,  Y0: 4.5822e+00,runtime: 241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1390,loss:2.2232e-02,  Y0: 4.5847e+00,runtime: 243\n",
      "step: 1400,loss:2.2103e-02,  Y0: 4.5866e+00,runtime: 245\n",
      "step: 1410,loss:2.2144e-02,  Y0: 4.5877e+00,runtime: 246\n",
      "step: 1420,loss:2.2055e-02,  Y0: 4.5881e+00,runtime: 248\n",
      "step: 1430,loss:2.1980e-02,  Y0: 4.5884e+00,runtime: 250\n",
      "step: 1440,loss:2.1805e-02,  Y0: 4.5897e+00,runtime: 252\n",
      "step: 1450,loss:2.1643e-02,  Y0: 4.5915e+00,runtime: 253\n",
      "step: 1460,loss:2.1548e-02,  Y0: 4.5930e+00,runtime: 255\n",
      "step: 1470,loss:2.1549e-02,  Y0: 4.5939e+00,runtime: 257\n",
      "step: 1480,loss:2.1457e-02,  Y0: 4.5945e+00,runtime: 259\n",
      "step: 1490,loss:2.1257e-02,  Y0: 4.5956e+00,runtime: 260\n",
      "step: 1500,loss:2.1147e-02,  Y0: 4.5960e+00,runtime: 262\n",
      "step: 1510,loss:2.0944e-02,  Y0: 4.5958e+00,runtime: 264\n",
      "step: 1520,loss:2.0907e-02,  Y0: 4.5967e+00,runtime: 265\n",
      "step: 1530,loss:2.0919e-02,  Y0: 4.5981e+00,runtime: 267\n",
      "step: 1540,loss:2.1063e-02,  Y0: 4.5985e+00,runtime: 269\n",
      "step: 1550,loss:2.1211e-02,  Y0: 4.5983e+00,runtime: 271\n",
      "step: 1560,loss:2.1331e-02,  Y0: 4.5984e+00,runtime: 272\n",
      "step: 1570,loss:2.1289e-02,  Y0: 4.5987e+00,runtime: 274\n",
      "step: 1580,loss:2.1361e-02,  Y0: 4.5986e+00,runtime: 276\n",
      "step: 1590,loss:2.1222e-02,  Y0: 4.5981e+00,runtime: 277\n",
      "step: 1600,loss:2.1294e-02,  Y0: 4.5977e+00,runtime: 279\n",
      "step: 1610,loss:2.1526e-02,  Y0: 4.5977e+00,runtime: 281\n",
      "step: 1620,loss:2.1679e-02,  Y0: 4.5987e+00,runtime: 283\n",
      "step: 1630,loss:2.1657e-02,  Y0: 4.5993e+00,runtime: 284\n",
      "step: 1640,loss:2.1576e-02,  Y0: 4.5988e+00,runtime: 286\n",
      "step: 1650,loss:2.1604e-02,  Y0: 4.5985e+00,runtime: 288\n",
      "step: 1660,loss:2.1598e-02,  Y0: 4.5982e+00,runtime: 289\n",
      "step: 1670,loss:2.1479e-02,  Y0: 4.5984e+00,runtime: 291\n",
      "step: 1680,loss:2.1396e-02,  Y0: 4.5990e+00,runtime: 293\n",
      "step: 1690,loss:2.1521e-02,  Y0: 4.5994e+00,runtime: 294\n",
      "step: 1700,loss:2.1457e-02,  Y0: 4.5990e+00,runtime: 296\n",
      "step: 1710,loss:2.1303e-02,  Y0: 4.5988e+00,runtime: 298\n",
      "step: 1720,loss:2.1329e-02,  Y0: 4.5993e+00,runtime: 299\n",
      "step: 1730,loss:2.1461e-02,  Y0: 4.6000e+00,runtime: 301\n",
      "step: 1740,loss:2.1344e-02,  Y0: 4.6005e+00,runtime: 303\n",
      "step: 1750,loss:2.1240e-02,  Y0: 4.6012e+00,runtime: 304\n",
      "step: 1760,loss:2.1285e-02,  Y0: 4.6021e+00,runtime: 306\n",
      "step: 1770,loss:2.1393e-02,  Y0: 4.6023e+00,runtime: 308\n",
      "step: 1780,loss:2.1326e-02,  Y0: 4.6023e+00,runtime: 309\n",
      "step: 1790,loss:2.1253e-02,  Y0: 4.6014e+00,runtime: 311\n",
      "step: 1800,loss:2.1282e-02,  Y0: 4.5996e+00,runtime: 313\n",
      "step: 1810,loss:2.1405e-02,  Y0: 4.5991e+00,runtime: 314\n",
      "step: 1820,loss:2.1500e-02,  Y0: 4.5990e+00,runtime: 316\n",
      "step: 1830,loss:2.1532e-02,  Y0: 4.5985e+00,runtime: 318\n",
      "step: 1840,loss:2.1471e-02,  Y0: 4.5976e+00,runtime: 319\n",
      "step: 1850,loss:2.1356e-02,  Y0: 4.5970e+00,runtime: 321\n",
      "step: 1860,loss:2.1488e-02,  Y0: 4.5968e+00,runtime: 323\n",
      "step: 1870,loss:2.1433e-02,  Y0: 4.5964e+00,runtime: 324\n",
      "step: 1880,loss:2.1330e-02,  Y0: 4.5966e+00,runtime: 326\n",
      "step: 1890,loss:2.1241e-02,  Y0: 4.5965e+00,runtime: 328\n",
      "step: 1900,loss:2.1272e-02,  Y0: 4.5974e+00,runtime: 329\n",
      "step: 1910,loss:2.1062e-02,  Y0: 4.5977e+00,runtime: 331\n",
      "step: 1920,loss:2.0833e-02,  Y0: 4.5978e+00,runtime: 333\n",
      "step: 1930,loss:2.0733e-02,  Y0: 4.5979e+00,runtime: 335\n",
      "step: 1940,loss:2.0558e-02,  Y0: 4.5987e+00,runtime: 336\n",
      "step: 1950,loss:2.0378e-02,  Y0: 4.5980e+00,runtime: 338\n",
      "step: 1960,loss:2.0379e-02,  Y0: 4.5972e+00,runtime: 340\n",
      "step: 1970,loss:2.0426e-02,  Y0: 4.5961e+00,runtime: 341\n",
      "step: 1980,loss:2.0705e-02,  Y0: 4.5963e+00,runtime: 343\n",
      "step: 1990,loss:2.1073e-02,  Y0: 4.5961e+00,runtime: 345\n",
      "step: 2000,loss:2.1296e-02,  Y0: 4.5974e+00,runtime: 346\n",
      "running time:347.080 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.python.training.moving_averages import assign_moving_average\n",
    "from scipy.stats import multivariate_normal as normal\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from tensorflow import random_normal_initializer as norm_init\n",
    "from tensorflow import random_uniform_initializer as unif_init\n",
    "from tensorflow import constant_initializer as const_init\n",
    "\n",
    "class SolveHJB(object):\n",
    "    def __init__(self, sess):\n",
    "     self.sess = sess\n",
    "     self.d = 100\n",
    "     self.T = 1\n",
    "     self.n_time = 20\n",
    "     self.n_layer =4\n",
    "     self.n_neuron = [self.d, self.d+10, self.d+10, self.d]\n",
    "     self.batch_size = 64\n",
    "     self.valid_size = 256\n",
    "     self.n_maxstep = 2000\n",
    "     self.n_displaystep = 10\n",
    "     self.learning_rate = 1/100\n",
    "     self.Yini = [0.3, 0.6]\n",
    "     self.h = (self.T+0.0)/self.n_time\n",
    "     self.sqrth = math.sqrt(self.h)\n",
    "     self.t_stamp = np.arange(0, self.n_time)*self.h\n",
    "     self._extra_train_ops = []\n",
    "     self._lambda = 1.0\n",
    "     \n",
    "    def train(self):\n",
    "     start_time = time.time()\n",
    "     self.global_step = tf.get_variable('global_step', [],initializer=tf.constant_initializer(1),trainable=False , dtype=tf.int32)\n",
    "     trainable_vars = tf.trainable_variables()\n",
    "     grads = tf.gradients(self.loss, trainable_vars)\n",
    "     optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "     apply_op = optimizer.apply_gradients(zip(grads, trainable_vars),global_step=self.global_step)\n",
    "     train_ops = [apply_op] + self._extra_train_ops\n",
    "     self.train_op = tf.group(*train_ops)\n",
    "     self.loss_history =[]\n",
    "     self.init_history =[]\n",
    "     dW_valid , X_valid= self.sample_path(self.valid_size) \n",
    "     feed_dict_valid ={self.dW: dW_valid , self.X: X_valid , self.is_training: False}\n",
    "     step=1\n",
    "     self.sess.run(tf.global_variables_initializer())\n",
    "     temp_loss = self.sess.run(self.loss, feed_dict=feed_dict_valid)\n",
    "     temp_init = self.Y0.eval()[0]\n",
    "     self.loss_history.append(temp_loss)\n",
    "     self.init_history.append(temp_init)\n",
    "     print(\"step: %5u, loss: %.4e, \"%(0, temp_loss)+\"Y0: %.4e,runtime: %4u\"%(temp_init,time.time()-start_time+self.t_bd))\n",
    "     for i in range(self.n_maxstep+1):\n",
    "         step = self.sess.run(self.global_step)\n",
    "         dW_train , X_train = self.sample_path(self.batch_size)\n",
    "         self.sess.run(self.train_op,feed_dict={self.dW: dW_train, self.X: X_train,self.is_training: True})\n",
    "         if  step % self.n_displaystep==0:\n",
    "             temp_loss = self.sess.run(self.loss,feed_dict=feed_dict_valid)\n",
    "             temp_init = self.Y0.eval()[0]\n",
    "             self.loss_history.append(temp_loss)\n",
    "             self.init_history.append(temp_init)\n",
    "             print(\"step:%5u,loss:%.4e, \" %(step,temp_loss), \"Y0:% .4e,runtime:% 4u\" %(temp_init,time.time()-start_time+self.t_bd))\n",
    "         step += 1\n",
    "     end_time = time.time()\n",
    "     print(\"running time:%.3f s\"%(end_time -start_time+self.t_bd))\n",
    "             \n",
    "    def build(self):\n",
    "                 start_time = time.time()\n",
    "                 self.dW =tf.placeholder(tf.float64 ,[None, self.d, self.n_time], name='dW')\n",
    "                 self.X = tf.placeholder(tf.float64 ,[None, self.d, self.n_time+1],name='X')\n",
    "                 self.is_training = tf.placeholder(tf.bool)\n",
    "                 self.Y0 = tf.Variable(tf.random_uniform([1],minval=self.Yini[0],maxval=self.Yini[1],dtype=tf.float64));\n",
    "                 self.Z0 = tf.Variable(tf.random_uniform([1, self.d],minval=-.1,maxval=.1,dtype=tf.float64))\n",
    "                 self.allones = tf.ones(shape=[tf.shape(self.dW)[0], 1],dtype=tf.float64)\n",
    "                 Y= self.allones * self.Y0\n",
    "                 Z= tf.matmul(self.allones , self.Z0)\n",
    "                 with tf.variable_scope('forward', reuse=tf.AUTO_REUSE):\n",
    "                     for t in range(0,self.n_time-1):\n",
    "                         Y= Y - self.f_tf(self.t_stamp[t],self.X[:, :, t], Y, Z)*self.h\n",
    "                         Y= Y + tf.reduce_sum(Z*self.dW[:, :, t], 1,keep_dims=True)\n",
    "                         Z = self._one_time_net(self.X[:, :, t+1],str(t+1))/self.d\n",
    "                     Y = Y - self.f_tf(self.t_stamp[self.n_time -1],self.X[:, :, self.n_time -1],Y, Z)*self.h\n",
    "                     Y = Y + tf.reduce_sum(Z*self.dW[:, :, self.n_time-1], 1,keep_dims=True)\n",
    "                     term_delta = Y - self.g_tf(self.T,self.X[:, :, self.n_time])\n",
    "                     self.clipped_delta = tf.clip_by_value(term_delta , -50.0, 50.0)\n",
    "                     self.loss = tf.reduce_mean(self.clipped_delta**2)\n",
    "                     self.t_bd = time.time()-start_time\n",
    "                \n",
    "    def sample_path(self, n_sample):\n",
    "        dW_sample = np.zeros([n_sample , self.d, self.n_time])\n",
    "        X_sample = np.zeros([n_sample, self.d, self.n_time+1])\n",
    "        for i in range(self.n_time):\n",
    "            dW_sample[:, :, i] = np.reshape(normal.rvs(mean=np.zeros(self.d),cov=1,size=n_sample)*self.sqrth,(n_sample, self.d))\n",
    "            X_sample[:, :, i+1] = X_sample[:, :, i] + np.sqrt(2) * dW_sample[:, :, i]\n",
    "        return dW_sample , X_sample\n",
    "        \n",
    "   # def f_tf(self , t, X,  Y, Z):\n",
    "   #    return Y-tf.pow(Y, 3)\n",
    "   # def g_tf(self, t, X):\n",
    "   #    return 0.5/(1 + 0.2*tf.reduce_sum(X**2, 1, keep_dims=True))\n",
    "    \n",
    "    def f_tf(self, t, x, y, z):\n",
    "        return -self._lambda * tf.reduce_sum(tf.square(z), 1, keep_dims=True)\n",
    "\n",
    "    def g_tf(self, t, x):\n",
    "        return tf.log((1 + tf.reduce_sum(tf.square(x), 1, keep_dims=True)) / 2)\n",
    "   \n",
    "    \n",
    "    def _one_time_net(self, x, name):\n",
    "       with tf.variable_scope(name):\n",
    "           x_norm = self._batch_norm(x, name='layer0_normal')\n",
    "           layer1 = self._one_layer(x_norm, self.n_neuron[1],name='layer1')\n",
    "           layer2 = self._one_layer(layer1, self.n_neuron[2],name='layer2')\n",
    "           z = self._one_layer(layer2, self.n_neuron[3], activation_fn=None, name='final')\n",
    "       return z\n",
    "       \n",
    "        \n",
    "    def _one_layer(self, input_, out_sz,activation_fn=tf.nn.relu,std=5.0, name='linear'):\n",
    "            with tf.variable_scope(name):\n",
    "                shape = input_.get_shape().as_list()\n",
    "                w = tf.get_variable('Matrix',[shape[1], out_sz],tf.float64 , norm_init(stddev= std/np.sqrt(shape[1]+out_sz))) \n",
    "                hidden=tf.matmul(input_ , w)\n",
    "                hidden_bn = self._batch_norm(hidden, name='normal')\n",
    "            if activation_fn != None:\n",
    "                    return activation_fn(hidden_bn)\n",
    "            else:\n",
    "                    return hidden_bn\n",
    "                \n",
    "    def _batch_norm(self, x, name):\n",
    "            with tf.variable_scope(name):\n",
    "                params_shape = [x.get_shape()[-1]]\n",
    "                beta = tf.get_variable('beta', params_shape , tf.float64 ,norm_init(0.0, stddev=0.1,dtype=tf.float64))\n",
    "                gamma =tf.get_variable('gamma', params_shape , tf.float64 ,unif_init(0.1, 0.5, dtype=tf.float64))\n",
    "                mv_mean = tf.get_variable('moving_mean', params_shape,tf.float64,const_init(0.0, tf.float64), trainable=False)\n",
    "                mv_var = tf.get_variable('moving_variance', params_shape,tf.float64,const_init(1.0, tf.float64), trainable=False)\n",
    "                mean, variance = tf.nn.moments(x, [0], name='moments')\n",
    "                self._extra_train_ops.append(assign_moving_average(mv_mean, mean, 0.99))\n",
    "                self._extra_train_ops.append(assign_moving_average(mv_var , variance , 0.99))\n",
    "                mean , variance = control_flow_ops.cond(self.is_training ,lambda: (mean, variance),lambda: (mv_mean , mv_var))\n",
    "                y = tf.nn.batch_normalization(x, mean, variance, beta, gamma, 1e-6)\n",
    "                y.set_shape(x.get_shape())\n",
    "                return y\n",
    "        \n",
    "def hjb_training(filename=\"./HJB_d10.csv\", display_step=10):\n",
    "       tf.reset_default_graph()\n",
    "       with tf.Session() as sess:\n",
    "           tf.set_random_seed(1)\n",
    "           print (\"Begin to solve HJB\")\n",
    "           model = SolveHJB(sess)\n",
    "           model.build()\n",
    "           #for m in range(1, 2000,200):\n",
    "           #print (\"m = \" + str(m))\n",
    "           model.n_displaystep = display_step\n",
    "           model.train()\n",
    "           output = np.zeros((len(model.init_history), 3))\n",
    "           output[:, 1] = model.loss_history \n",
    "                      \n",
    "           np.savetxt(filename, output ,fmt=['%d', '%.5e', '%.5e'], delimiter=\",\",header=\"step, loss function, \" +  \"target value ,runtime\", comments='')\n",
    "           return output\n",
    "if __name__ == '__main__':\n",
    "               np.random.seed(1) \n",
    "               hjb_training()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin to solve HJB\n",
      "step:     0, loss: 1.7803e+01, Y0: 3.5068e-01,runtime:   38\n",
      "step:  100,loss:4.9219e+00,  Y0: 1.2538e+00,runtime:  62\n",
      "step:  200,loss:2.4632e+00,  Y0: 1.5661e+00,runtime:  74\n",
      "step:  300,loss:2.2577e+00,  Y0: 1.7798e+00,runtime:  89\n",
      "step:  400,loss:2.0437e+00,  Y0: 2.0286e+00,runtime: 106\n",
      "step:  500,loss:1.8501e+00,  Y0: 2.3142e+00,runtime: 124\n"
     ]
    }
   ],
   "source": [
    "hjb_training(\"./HJB_d100.csv\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous traçons la fonction de perte avec 200 époques de 10 itérations. On remarque que l'algorithme diminue rapidement la perte avant d'atteindre un palier symbole de stagnation de l'apprentissage. Nous traçons également le graphique basé sur 20 époques de 100 itérations. \n",
    "\n",
    "Nous observons qu'avec un nombre d'itérations moins important dans le minibatch, la perte diminue plus vite. \n",
    "Par contre le palier de stagnation est atteint à peu près au même moment dans les deux graphiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "loss_history = pd.read_csv(\"HJB_d10.csv\")\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(121)\n",
    "plt.plot(loss_history.iloc[1:,1].values)\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "loss_history2 = pd.read_csv(\"HJB_d100.csv\")\n",
    "plt.subplot(122)\n",
    "plt.plot(loss_history2.iloc[1:,1].values)\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

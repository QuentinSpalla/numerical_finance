{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Résolution d'EDP par réseau de neurones dans le cadre HJB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous reprenons l'algorithme codé par les auteurs dans le cadre Allen-Cahn (partie 4.2 de l'article) pour l'adapter au cadre HJB (partie 4.3 de l'article).\n",
    "\n",
    "L'algorithme de l'article s'intéresse à la résolution de l'équation différentielle :\n",
    "\n",
    "$$\\frac{\\partial_u}{\\partial_t}(t,x)+ \\frac{1}{2}Trace\\Big(\\sigma(t,x)[\\sigma(t,x)]*(Hess_x u)(t,x)\\Big) + <\\mu(t,x),(\\nabla_x u)(t,x)> +f\\Big(t,x,u(t,x),[(\\nabla_x u)(t,x)]*\\sigma (t,x)\\Big) = 0$$\n",
    "\n",
    "En particulier, dans le cadre HJB :\n",
    "$$f(t,x,y,z) = -\\vert \\vert z \\vert \\vert ^2$$\n",
    "\n",
    "$$g(x) = ln(\\frac{1}{2}[1+\\vert \\vert x \\vert \\vert ^2] )$$\n",
    "\n",
    "$$\\sigma(t,x)w = \\sqrt{2}w$$\n",
    "\n",
    "$$\\mu (t,x) = 0$$\n",
    "\n",
    "La solution de l'EDP est alors  :\n",
    "\n",
    "$$\\frac{\\partial_u}{\\partial_t}(t,x)+ (\\Delta_x u)(t,x) = \\vert \\vert (\\nabla_x u)(t,x)\\vert \\vert ^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin to solve HJB\n",
      "step:     0, loss: 1.7901e+01, Y0: 3.5068e-01,runtime:   33\n",
      "step:   10,loss:1.6998e+01,  Y0: 4.5058e-01,runtime:  46\n",
      "step:   20,loss:1.6026e+01,  Y0: 5.4995e-01,runtime:  47\n",
      "step:   30,loss:1.4992e+01,  Y0: 6.4840e-01,runtime:  48\n",
      "step:   40,loss:1.3861e+01,  Y0: 7.4537e-01,runtime:  50\n",
      "step:   50,loss:1.2631e+01,  Y0: 8.4032e-01,runtime:  51\n",
      "step:   60,loss:1.1237e+01,  Y0: 9.3260e-01,runtime:  53\n",
      "step:   70,loss:9.7102e+00,  Y0: 1.0218e+00,runtime:  54\n",
      "step:   80,loss:8.1462e+00,  Y0: 1.1078e+00,runtime:  55\n",
      "step:   90,loss:6.5926e+00,  Y0: 1.1883e+00,runtime:  57\n",
      "step:  100,loss:5.1831e+00,  Y0: 1.2623e+00,runtime:  58\n",
      "step:  110,loss:4.0403e+00,  Y0: 1.3280e+00,runtime:  59\n",
      "step:  120,loss:3.2666e+00,  Y0: 1.3833e+00,runtime:  61\n",
      "step:  130,loss:2.8419e+00,  Y0: 1.4282e+00,runtime:  62\n",
      "step:  140,loss:2.6667e+00,  Y0: 1.4635e+00,runtime:  63\n",
      "step:  150,loss:2.6090e+00,  Y0: 1.4923e+00,runtime:  65\n",
      "step:  160,loss:2.5925e+00,  Y0: 1.5128e+00,runtime:  66\n",
      "step:  170,loss:2.5812e+00,  Y0: 1.5315e+00,runtime:  67\n",
      "step:  180,loss:2.5627e+00,  Y0: 1.5479e+00,runtime:  68\n",
      "step:  190,loss:2.5373e+00,  Y0: 1.5646e+00,runtime:  70\n",
      "step:  200,loss:2.5152e+00,  Y0: 1.5813e+00,runtime:  71\n",
      "step:  210,loss:2.4959e+00,  Y0: 1.6002e+00,runtime:  73\n",
      "step:  220,loss:2.4784e+00,  Y0: 1.6193e+00,runtime:  74\n",
      "step:  230,loss:2.4687e+00,  Y0: 1.6385e+00,runtime:  75\n",
      "step:  240,loss:2.4535e+00,  Y0: 1.6561e+00,runtime:  77\n",
      "step:  250,loss:2.4319e+00,  Y0: 1.6761e+00,runtime:  78\n",
      "step:  260,loss:2.4082e+00,  Y0: 1.6985e+00,runtime:  79\n",
      "step:  270,loss:2.3886e+00,  Y0: 1.7206e+00,runtime:  81\n",
      "step:  280,loss:2.3691e+00,  Y0: 1.7415e+00,runtime:  82\n",
      "step:  290,loss:2.3478e+00,  Y0: 1.7625e+00,runtime:  83\n",
      "step:  300,loss:2.3248e+00,  Y0: 1.7827e+00,runtime:  84\n",
      "step:  310,loss:2.3116e+00,  Y0: 1.8040e+00,runtime:  86\n",
      "step:  320,loss:2.2907e+00,  Y0: 1.8294e+00,runtime:  87\n",
      "step:  330,loss:2.2699e+00,  Y0: 1.8590e+00,runtime:  88\n",
      "step:  340,loss:2.2600e+00,  Y0: 1.8835e+00,runtime:  90\n",
      "step:  350,loss:2.2440e+00,  Y0: 1.9083e+00,runtime:  91\n",
      "step:  360,loss:2.2226e+00,  Y0: 1.9340e+00,runtime:  92\n",
      "step:  370,loss:2.1939e+00,  Y0: 1.9607e+00,runtime:  94\n",
      "step:  380,loss:2.1619e+00,  Y0: 1.9861e+00,runtime:  95\n",
      "step:  390,loss:2.1416e+00,  Y0: 2.0100e+00,runtime:  96\n",
      "step:  400,loss:2.1181e+00,  Y0: 2.0332e+00,runtime:  97\n",
      "step:  410,loss:2.0907e+00,  Y0: 2.0598e+00,runtime:  99\n",
      "step:  420,loss:2.0658e+00,  Y0: 2.0875e+00,runtime: 100\n",
      "step:  430,loss:2.0422e+00,  Y0: 2.1153e+00,runtime: 102\n",
      "step:  440,loss:2.0086e+00,  Y0: 2.1459e+00,runtime: 103\n",
      "step:  450,loss:1.9769e+00,  Y0: 2.1790e+00,runtime: 104\n",
      "step:  460,loss:1.9502e+00,  Y0: 2.2089e+00,runtime: 105\n",
      "step:  470,loss:1.9230e+00,  Y0: 2.2376e+00,runtime: 107\n",
      "step:  480,loss:1.8865e+00,  Y0: 2.2635e+00,runtime: 108\n",
      "step:  490,loss:1.8566e+00,  Y0: 2.2886e+00,runtime: 109\n",
      "step:  500,loss:1.8295e+00,  Y0: 2.3186e+00,runtime: 110\n",
      "step:  510,loss:1.8019e+00,  Y0: 2.3505e+00,runtime: 111\n",
      "step:  520,loss:1.7740e+00,  Y0: 2.3788e+00,runtime: 113\n",
      "step:  530,loss:1.7483e+00,  Y0: 2.4071e+00,runtime: 114\n",
      "step:  540,loss:1.7229e+00,  Y0: 2.4364e+00,runtime: 116\n",
      "step:  550,loss:1.7012e+00,  Y0: 2.4659e+00,runtime: 117\n",
      "step:  560,loss:1.6766e+00,  Y0: 2.4982e+00,runtime: 119\n",
      "step:  570,loss:1.6479e+00,  Y0: 2.5313e+00,runtime: 120\n",
      "step:  580,loss:1.6223e+00,  Y0: 2.5627e+00,runtime: 121\n",
      "step:  590,loss:1.5945e+00,  Y0: 2.5943e+00,runtime: 122\n",
      "step:  600,loss:1.5622e+00,  Y0: 2.6266e+00,runtime: 123\n",
      "step:  610,loss:1.5353e+00,  Y0: 2.6569e+00,runtime: 124\n",
      "step:  620,loss:1.5159e+00,  Y0: 2.6889e+00,runtime: 126\n",
      "step:  630,loss:1.4902e+00,  Y0: 2.7235e+00,runtime: 127\n",
      "step:  640,loss:1.4646e+00,  Y0: 2.7612e+00,runtime: 128\n",
      "step:  650,loss:1.4320e+00,  Y0: 2.7918e+00,runtime: 129\n",
      "step:  660,loss:1.4022e+00,  Y0: 2.8195e+00,runtime: 131\n",
      "step:  670,loss:1.3677e+00,  Y0: 2.8498e+00,runtime: 132\n",
      "step:  680,loss:1.3306e+00,  Y0: 2.8833e+00,runtime: 133\n",
      "step:  690,loss:1.2945e+00,  Y0: 2.9204e+00,runtime: 135\n",
      "step:  700,loss:1.2622e+00,  Y0: 2.9564e+00,runtime: 136\n",
      "step:  710,loss:1.2302e+00,  Y0: 2.9919e+00,runtime: 138\n",
      "step:  720,loss:1.2020e+00,  Y0: 3.0275e+00,runtime: 140\n",
      "step:  730,loss:1.1757e+00,  Y0: 3.0618e+00,runtime: 142\n",
      "step:  740,loss:1.1481e+00,  Y0: 3.0945e+00,runtime: 143\n",
      "step:  750,loss:1.1224e+00,  Y0: 3.1267e+00,runtime: 145\n",
      "step:  760,loss:1.0915e+00,  Y0: 3.1572e+00,runtime: 147\n",
      "step:  770,loss:1.0625e+00,  Y0: 3.1908e+00,runtime: 149\n",
      "step:  780,loss:1.0365e+00,  Y0: 3.2306e+00,runtime: 151\n",
      "step:  790,loss:1.0075e+00,  Y0: 3.2695e+00,runtime: 153\n",
      "step:  800,loss:9.7844e-01,  Y0: 3.3044e+00,runtime: 154\n",
      "step:  810,loss:9.5103e-01,  Y0: 3.3376e+00,runtime: 156\n",
      "step:  820,loss:9.2451e-01,  Y0: 3.3716e+00,runtime: 158\n",
      "step:  830,loss:8.9299e-01,  Y0: 3.4058e+00,runtime: 159\n",
      "step:  840,loss:8.6459e-01,  Y0: 3.4412e+00,runtime: 161\n",
      "step:  850,loss:8.3131e-01,  Y0: 3.4792e+00,runtime: 163\n",
      "step:  860,loss:8.0115e-01,  Y0: 3.5156e+00,runtime: 165\n",
      "step:  870,loss:7.7105e-01,  Y0: 3.5498e+00,runtime: 167\n",
      "step:  880,loss:7.4254e-01,  Y0: 3.5841e+00,runtime: 168\n",
      "step:  890,loss:7.1182e-01,  Y0: 3.6205e+00,runtime: 170\n",
      "step:  900,loss:6.8289e-01,  Y0: 3.6556e+00,runtime: 172\n",
      "step:  910,loss:6.5571e-01,  Y0: 3.6904e+00,runtime: 174\n",
      "step:  920,loss:6.2378e-01,  Y0: 3.7262e+00,runtime: 176\n",
      "step:  930,loss:5.9242e-01,  Y0: 3.7623e+00,runtime: 177\n",
      "step:  940,loss:5.6090e-01,  Y0: 3.7959e+00,runtime: 179\n",
      "step:  950,loss:5.3267e-01,  Y0: 3.8294e+00,runtime: 181\n",
      "step:  960,loss:5.0542e-01,  Y0: 3.8628e+00,runtime: 183\n",
      "step:  970,loss:4.7469e-01,  Y0: 3.8948e+00,runtime: 185\n",
      "step:  980,loss:4.4518e-01,  Y0: 3.9270e+00,runtime: 187\n",
      "step:  990,loss:4.1740e-01,  Y0: 3.9598e+00,runtime: 189\n",
      "step: 1000,loss:3.8842e-01,  Y0: 3.9906e+00,runtime: 190\n",
      "step: 1010,loss:3.6228e-01,  Y0: 4.0208e+00,runtime: 192\n",
      "step: 1020,loss:3.3881e-01,  Y0: 4.0517e+00,runtime: 194\n",
      "step: 1030,loss:3.1486e-01,  Y0: 4.0814e+00,runtime: 196\n",
      "step: 1040,loss:2.9087e-01,  Y0: 4.1103e+00,runtime: 198\n",
      "step: 1050,loss:2.6850e-01,  Y0: 4.1385e+00,runtime: 200\n",
      "step: 1060,loss:2.4749e-01,  Y0: 4.1656e+00,runtime: 202\n",
      "step: 1070,loss:2.2945e-01,  Y0: 4.1915e+00,runtime: 204\n",
      "step: 1080,loss:2.1185e-01,  Y0: 4.2166e+00,runtime: 206\n",
      "step: 1090,loss:1.9288e-01,  Y0: 4.2403e+00,runtime: 208\n",
      "step: 1100,loss:1.7420e-01,  Y0: 4.2628e+00,runtime: 210\n",
      "step: 1110,loss:1.5698e-01,  Y0: 4.2870e+00,runtime: 211\n",
      "step: 1120,loss:1.4165e-01,  Y0: 4.3098e+00,runtime: 213\n",
      "step: 1130,loss:1.2787e-01,  Y0: 4.3298e+00,runtime: 215\n",
      "step: 1140,loss:1.1507e-01,  Y0: 4.3480e+00,runtime: 217\n",
      "step: 1150,loss:1.0280e-01,  Y0: 4.3668e+00,runtime: 219\n",
      "step: 1160,loss:9.2435e-02,  Y0: 4.3858e+00,runtime: 221\n",
      "step: 1170,loss:8.2829e-02,  Y0: 4.4047e+00,runtime: 223\n",
      "step: 1180,loss:7.3469e-02,  Y0: 4.4226e+00,runtime: 225\n",
      "step: 1190,loss:6.5274e-02,  Y0: 4.4382e+00,runtime: 226\n",
      "step: 1200,loss:5.7838e-02,  Y0: 4.4541e+00,runtime: 228\n",
      "step: 1210,loss:5.1534e-02,  Y0: 4.4681e+00,runtime: 230\n",
      "step: 1220,loss:4.6080e-02,  Y0: 4.4809e+00,runtime: 232\n",
      "step: 1230,loss:4.1444e-02,  Y0: 4.4936e+00,runtime: 234\n",
      "step: 1240,loss:3.7971e-02,  Y0: 4.5053e+00,runtime: 236\n",
      "step: 1250,loss:3.5024e-02,  Y0: 4.5161e+00,runtime: 238\n",
      "step: 1260,loss:3.2755e-02,  Y0: 4.5248e+00,runtime: 240\n",
      "step: 1270,loss:3.0770e-02,  Y0: 4.5323e+00,runtime: 241\n",
      "step: 1280,loss:2.9068e-02,  Y0: 4.5392e+00,runtime: 243\n",
      "step: 1290,loss:2.7424e-02,  Y0: 4.5451e+00,runtime: 245\n",
      "step: 1300,loss:2.5933e-02,  Y0: 4.5509e+00,runtime: 248\n",
      "step: 1310,loss:2.4827e-02,  Y0: 4.5563e+00,runtime: 249\n",
      "step: 1320,loss:2.4059e-02,  Y0: 4.5606e+00,runtime: 251\n",
      "step: 1330,loss:2.3578e-02,  Y0: 4.5645e+00,runtime: 253\n",
      "step: 1340,loss:2.3105e-02,  Y0: 4.5683e+00,runtime: 255\n",
      "step: 1350,loss:2.2644e-02,  Y0: 4.5718e+00,runtime: 257\n",
      "step: 1360,loss:2.2402e-02,  Y0: 4.5751e+00,runtime: 260\n",
      "step: 1370,loss:2.2183e-02,  Y0: 4.5786e+00,runtime: 263\n",
      "step: 1380,loss:2.2105e-02,  Y0: 4.5819e+00,runtime: 265\n",
      "step: 1390,loss:2.1974e-02,  Y0: 4.5844e+00,runtime: 267\n",
      "step: 1400,loss:2.1809e-02,  Y0: 4.5863e+00,runtime: 269\n",
      "step: 1410,loss:2.1818e-02,  Y0: 4.5874e+00,runtime: 271\n",
      "step: 1420,loss:2.1717e-02,  Y0: 4.5879e+00,runtime: 272\n",
      "step: 1430,loss:2.1647e-02,  Y0: 4.5882e+00,runtime: 274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1440,loss:2.1561e-02,  Y0: 4.5896e+00,runtime: 276\n",
      "step: 1450,loss:2.1458e-02,  Y0: 4.5915e+00,runtime: 278\n",
      "step: 1460,loss:2.1394e-02,  Y0: 4.5930e+00,runtime: 280\n",
      "step: 1470,loss:2.1458e-02,  Y0: 4.5939e+00,runtime: 282\n",
      "step: 1480,loss:2.1415e-02,  Y0: 4.5945e+00,runtime: 284\n",
      "step: 1490,loss:2.1248e-02,  Y0: 4.5956e+00,runtime: 286\n",
      "step: 1500,loss:2.1184e-02,  Y0: 4.5962e+00,runtime: 288\n",
      "step: 1510,loss:2.1048e-02,  Y0: 4.5960e+00,runtime: 290\n",
      "step: 1520,loss:2.1077e-02,  Y0: 4.5970e+00,runtime: 292\n",
      "step: 1530,loss:2.1104e-02,  Y0: 4.5984e+00,runtime: 294\n",
      "step: 1540,loss:2.1220e-02,  Y0: 4.5988e+00,runtime: 297\n",
      "step: 1550,loss:2.1372e-02,  Y0: 4.5988e+00,runtime: 299\n",
      "step: 1560,loss:2.1482e-02,  Y0: 4.5989e+00,runtime: 301\n",
      "step: 1570,loss:2.1400e-02,  Y0: 4.5992e+00,runtime: 302\n",
      "step: 1580,loss:2.1414e-02,  Y0: 4.5991e+00,runtime: 304\n",
      "step: 1590,loss:2.1230e-02,  Y0: 4.5987e+00,runtime: 306\n",
      "step: 1600,loss:2.1288e-02,  Y0: 4.5984e+00,runtime: 308\n",
      "step: 1610,loss:2.1513e-02,  Y0: 4.5982e+00,runtime: 310\n",
      "step: 1620,loss:2.1669e-02,  Y0: 4.5991e+00,runtime: 312\n",
      "step: 1630,loss:2.1648e-02,  Y0: 4.5996e+00,runtime: 314\n",
      "step: 1640,loss:2.1556e-02,  Y0: 4.5991e+00,runtime: 315\n",
      "step: 1650,loss:2.1559e-02,  Y0: 4.5987e+00,runtime: 317\n",
      "step: 1660,loss:2.1506e-02,  Y0: 4.5984e+00,runtime: 319\n",
      "step: 1670,loss:2.1406e-02,  Y0: 4.5987e+00,runtime: 321\n",
      "step: 1680,loss:2.1415e-02,  Y0: 4.5993e+00,runtime: 322\n",
      "step: 1690,loss:2.1622e-02,  Y0: 4.5999e+00,runtime: 325\n",
      "step: 1700,loss:2.1614e-02,  Y0: 4.5997e+00,runtime: 327\n",
      "step: 1710,loss:2.1480e-02,  Y0: 4.5995e+00,runtime: 329\n",
      "step: 1720,loss:2.1511e-02,  Y0: 4.5997e+00,runtime: 330\n",
      "step: 1730,loss:2.1628e-02,  Y0: 4.6002e+00,runtime: 332\n",
      "step: 1740,loss:2.1476e-02,  Y0: 4.6006e+00,runtime: 334\n",
      "step: 1750,loss:2.1343e-02,  Y0: 4.6012e+00,runtime: 336\n",
      "step: 1760,loss:2.1409e-02,  Y0: 4.6022e+00,runtime: 337\n",
      "step: 1770,loss:2.1459e-02,  Y0: 4.6026e+00,runtime: 339\n",
      "step: 1780,loss:2.1405e-02,  Y0: 4.6026e+00,runtime: 341\n",
      "step: 1790,loss:2.1357e-02,  Y0: 4.6017e+00,runtime: 343\n",
      "step: 1800,loss:2.1409e-02,  Y0: 4.5999e+00,runtime: 345\n",
      "step: 1810,loss:2.1515e-02,  Y0: 4.5991e+00,runtime: 347\n",
      "step: 1820,loss:2.1618e-02,  Y0: 4.5989e+00,runtime: 348\n",
      "step: 1830,loss:2.1660e-02,  Y0: 4.5984e+00,runtime: 350\n",
      "step: 1840,loss:2.1634e-02,  Y0: 4.5974e+00,runtime: 353\n",
      "step: 1850,loss:2.1572e-02,  Y0: 4.5969e+00,runtime: 355\n",
      "step: 1860,loss:2.1666e-02,  Y0: 4.5968e+00,runtime: 356\n",
      "step: 1870,loss:2.1574e-02,  Y0: 4.5965e+00,runtime: 358\n",
      "step: 1880,loss:2.1428e-02,  Y0: 4.5969e+00,runtime: 360\n",
      "step: 1890,loss:2.1313e-02,  Y0: 4.5969e+00,runtime: 362\n",
      "step: 1900,loss:2.1345e-02,  Y0: 4.5979e+00,runtime: 364\n",
      "step: 1910,loss:2.1182e-02,  Y0: 4.5983e+00,runtime: 366\n",
      "step: 1920,loss:2.1035e-02,  Y0: 4.5983e+00,runtime: 368\n",
      "step: 1930,loss:2.0950e-02,  Y0: 4.5983e+00,runtime: 370\n",
      "step: 1940,loss:2.0793e-02,  Y0: 4.5990e+00,runtime: 371\n",
      "step: 1950,loss:2.0613e-02,  Y0: 4.5983e+00,runtime: 373\n",
      "step: 1960,loss:2.0544e-02,  Y0: 4.5975e+00,runtime: 375\n",
      "step: 1970,loss:2.0573e-02,  Y0: 4.5964e+00,runtime: 377\n",
      "step: 1980,loss:2.0867e-02,  Y0: 4.5965e+00,runtime: 379\n",
      "step: 1990,loss:2.1323e-02,  Y0: 4.5964e+00,runtime: 381\n",
      "step: 2000,loss:2.1610e-02,  Y0: 4.5977e+00,runtime: 383\n",
      "running time:383.264 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.python.training.moving_averages import assign_moving_average\n",
    "from scipy.stats import multivariate_normal as normal\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from tensorflow import random_normal_initializer as norm_init\n",
    "from tensorflow import random_uniform_initializer as unif_init\n",
    "from tensorflow import constant_initializer as const_init\n",
    "\n",
    "class SolveHJB(object):\n",
    "    def __init__(self, sess):\n",
    "     self.sess = sess\n",
    "     self.d = 100\n",
    "     self.T = 1\n",
    "     self.n_time = 20\n",
    "     self.n_layer =4\n",
    "     self.n_neuron = [self.d, self.d+10, self.d+10, self.d]\n",
    "     self.batch_size = 64\n",
    "     self.valid_size = 256\n",
    "     self.n_maxstep = 2000\n",
    "     self.n_displaystep = 10\n",
    "     self.learning_rate = 1/100\n",
    "     self.Yini = [0.3, 0.6]\n",
    "     self.h = (self.T+0.0)/self.n_time\n",
    "     self.sqrth = math.sqrt(self.h)\n",
    "     self.t_stamp = np.arange(0, self.n_time)*self.h\n",
    "     self._extra_train_ops = []\n",
    "     self._lambda = 1.0\n",
    "     \n",
    "    def train(self):\n",
    "     start_time = time.time()\n",
    "     self.global_step = tf.get_variable('global_step', [],initializer=tf.constant_initializer(1),trainable=False , dtype=tf.int32)\n",
    "     trainable_vars = tf.trainable_variables()\n",
    "     grads = tf.gradients(self.loss, trainable_vars)\n",
    "     optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "     apply_op = optimizer.apply_gradients(zip(grads, trainable_vars),global_step=self.global_step)\n",
    "     train_ops = [apply_op] + self._extra_train_ops\n",
    "     self.train_op = tf.group(*train_ops)\n",
    "     self.loss_history =[]\n",
    "     self.init_history =[]\n",
    "     dW_valid , X_valid= self.sample_path(self.valid_size) \n",
    "     feed_dict_valid ={self.dW: dW_valid , self.X: X_valid , self.is_training: False}\n",
    "     step=1\n",
    "     self.sess.run(tf.global_variables_initializer())\n",
    "     temp_loss = self.sess.run(self.loss, feed_dict=feed_dict_valid)\n",
    "     temp_init = self.Y0.eval()[0]\n",
    "     self.loss_history.append(temp_loss)\n",
    "     self.init_history.append(temp_init)\n",
    "     print(\"step: %5u, loss: %.4e, \"%(0, temp_loss)+\"Y0: %.4e,runtime: %4u\"%(temp_init,time.time()-start_time+self.t_bd))\n",
    "     for i in range(self.n_maxstep+1):\n",
    "         step = self.sess.run(self.global_step)\n",
    "         dW_train , X_train = self.sample_path(self.batch_size)\n",
    "         self.sess.run(self.train_op,feed_dict={self.dW: dW_train, self.X: X_train,self.is_training: True})\n",
    "         if  step % self.n_displaystep==0:\n",
    "             temp_loss = self.sess.run(self.loss,feed_dict=feed_dict_valid)\n",
    "             temp_init = self.Y0.eval()[0]\n",
    "             self.loss_history.append(temp_loss)\n",
    "             self.init_history.append(temp_init)\n",
    "             print(\"step:%5u,loss:%.4e, \" %(step,temp_loss), \"Y0:% .4e,runtime:% 4u\" %(temp_init,time.time()-start_time+self.t_bd))\n",
    "         step += 1\n",
    "     end_time = time.time()\n",
    "     print(\"running time:%.3f s\"%(end_time -start_time+self.t_bd))\n",
    "             \n",
    "    def build(self):\n",
    "                 start_time = time.time()\n",
    "                 self.dW =tf.placeholder(tf.float64 ,[None, self.d, self.n_time], name='dW')\n",
    "                 self.X = tf.placeholder(tf.float64 ,[None, self.d, self.n_time+1],name='X')\n",
    "                 self.is_training = tf.placeholder(tf.bool)\n",
    "                 self.Y0 = tf.Variable(tf.random_uniform([1],minval=self.Yini[0],maxval=self.Yini[1],dtype=tf.float64));\n",
    "                 self.Z0 = tf.Variable(tf.random_uniform([1, self.d],minval=-.1,maxval=.1,dtype=tf.float64))\n",
    "                 self.allones = tf.ones(shape=[tf.shape(self.dW)[0], 1],dtype=tf.float64)\n",
    "                 Y= self.allones * self.Y0\n",
    "                 Z= tf.matmul(self.allones , self.Z0)\n",
    "                 with tf.variable_scope('forward', reuse=tf.AUTO_REUSE):\n",
    "                     for t in range(0,self.n_time-1):\n",
    "                         Y= Y - self.f_tf(self.t_stamp[t],self.X[:, :, t], Y, Z)*self.h\n",
    "                         Y= Y + tf.reduce_sum(Z*self.dW[:, :, t], 1,keep_dims=True)\n",
    "                         Z = self._one_time_net(self.X[:, :, t+1],str(t+1))/self.d\n",
    "                     Y = Y - self.f_tf(self.t_stamp[self.n_time -1],self.X[:, :, self.n_time -1],Y, Z)*self.h\n",
    "                     Y = Y + tf.reduce_sum(Z*self.dW[:, :, self.n_time-1], 1,keep_dims=True)\n",
    "                     term_delta = Y - self.g_tf(self.T,self.X[:, :, self.n_time])\n",
    "                     self.clipped_delta = tf.clip_by_value(term_delta , -50.0, 50.0)\n",
    "                     self.loss = tf.reduce_mean(self.clipped_delta**2)\n",
    "                     self.t_bd = time.time()-start_time\n",
    "                \n",
    "    def sample_path(self, n_sample):\n",
    "        dW_sample = np.zeros([n_sample , self.d, self.n_time])\n",
    "        X_sample = np.zeros([n_sample, self.d, self.n_time+1])\n",
    "        for i in range(self.n_time):\n",
    "            dW_sample[:, :, i] = np.reshape(normal.rvs(mean=np.zeros(self.d),cov=1,size=n_sample)*self.sqrth,(n_sample, self.d))\n",
    "            X_sample[:, :, i+1] = X_sample[:, :, i] + np.sqrt(2) * dW_sample[:, :, i]\n",
    "        return dW_sample , X_sample\n",
    "        \n",
    "   # def f_tf(self , t, X,  Y, Z):\n",
    "   #    return Y-tf.pow(Y, 3)\n",
    "   # def g_tf(self, t, X):\n",
    "   #    return 0.5/(1 + 0.2*tf.reduce_sum(X**2, 1, keep_dims=True))\n",
    "    \n",
    "    def f_tf(self, t, x, y, z):\n",
    "        return -self._lambda * tf.reduce_sum(tf.square(z), 1, keep_dims=True)\n",
    "\n",
    "    def g_tf(self, t, x):\n",
    "        return tf.log((1 + tf.reduce_sum(tf.square(x), 1, keep_dims=True)) / 2)\n",
    "   \n",
    "    \n",
    "    def _one_time_net(self, x, name):\n",
    "       with tf.variable_scope(name):\n",
    "           x_norm = self._batch_norm(x, name='layer0_normal')\n",
    "           layer1 = self._one_layer(x_norm, self.n_neuron[1],name='layer1')\n",
    "           layer2 = self._one_layer(layer1, self.n_neuron[2],name='layer2')\n",
    "           z = self._one_layer(layer2, self.n_neuron[3], activation_fn=None, name='final')\n",
    "       return z\n",
    "       \n",
    "        \n",
    "    def _one_layer(self, input_, out_sz,activation_fn=tf.nn.relu,std=5.0, name='linear'):\n",
    "            with tf.variable_scope(name):\n",
    "                shape = input_.get_shape().as_list()\n",
    "                w = tf.get_variable('Matrix',[shape[1], out_sz],tf.float64 , norm_init(stddev= std/np.sqrt(shape[1]+out_sz))) \n",
    "                hidden=tf.matmul(input_ , w)\n",
    "                hidden_bn = self._batch_norm(hidden, name='normal')\n",
    "            if activation_fn != None:\n",
    "                    return activation_fn(hidden_bn)\n",
    "            else:\n",
    "                    return hidden_bn\n",
    "                \n",
    "    def _batch_norm(self, x, name):\n",
    "            with tf.variable_scope(name):\n",
    "                params_shape = [x.get_shape()[-1]]\n",
    "                beta = tf.get_variable('beta', params_shape , tf.float64 ,norm_init(0.0, stddev=0.1,dtype=tf.float64))\n",
    "                gamma =tf.get_variable('gamma', params_shape , tf.float64 ,unif_init(0.1, 0.5, dtype=tf.float64))\n",
    "                mv_mean = tf.get_variable('moving_mean', params_shape,tf.float64,const_init(0.0, tf.float64), trainable=False)\n",
    "                mv_var = tf.get_variable('moving_variance', params_shape,tf.float64,const_init(1.0, tf.float64), trainable=False)\n",
    "                mean, variance = tf.nn.moments(x, [0], name='moments')\n",
    "                self._extra_train_ops.append(assign_moving_average(mv_mean, mean, 0.99))\n",
    "                self._extra_train_ops.append(assign_moving_average(mv_var , variance , 0.99))\n",
    "                mean , variance = control_flow_ops.cond(self.is_training ,lambda: (mean, variance),lambda: (mv_mean , mv_var))\n",
    "                y = tf.nn.batch_normalization(x, mean, variance, beta, gamma, 1e-6)\n",
    "                y.set_shape(x.get_shape())\n",
    "                return y\n",
    "        \n",
    "def hjb_training(filename=\"./HJB_d10.csv\", display_step=10):\n",
    "       tf.reset_default_graph()\n",
    "       with tf.Session() as sess:\n",
    "           tf.set_random_seed(1)\n",
    "           print (\"Begin to solve HJB\")\n",
    "           model = SolveHJB(sess)\n",
    "           model.build()\n",
    "           #for m in range(1, 2000,200):\n",
    "           #print (\"m = \" + str(m))\n",
    "           model.n_displaystep = display_step\n",
    "           model.train()\n",
    "           output = np.zeros((len(model.init_history), 3))\n",
    "           output[:, 1] = model.loss_history \n",
    "                      \n",
    "           np.savetxt(filename, output ,fmt=['%d', '%.5e', '%.5e'], delimiter=\",\",header=\"step, loss function, \" +  \"target value ,runtime\", comments='')\n",
    "           return output\n",
    "if __name__ == '__main__':\n",
    "               np.random.seed(1) \n",
    "               hjb_training()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin to solve HJB\n",
      "step:     0, loss: 1.7803e+01, Y0: 3.5068e-01,runtime:   48\n",
      "step:  100,loss:4.9231e+00,  Y0: 1.2538e+00,runtime:  75\n",
      "step:  200,loss:2.4649e+00,  Y0: 1.5661e+00,runtime:  87\n",
      "step:  300,loss:2.2619e+00,  Y0: 1.7798e+00,runtime: 101\n",
      "step:  400,loss:2.0495e+00,  Y0: 2.0286e+00,runtime: 118\n",
      "step:  500,loss:1.8578e+00,  Y0: 2.3143e+00,runtime: 136\n",
      "step:  600,loss:1.5635e+00,  Y0: 2.6290e+00,runtime: 154\n",
      "step:  700,loss:1.2369e+00,  Y0: 2.9651e+00,runtime: 171\n",
      "step:  800,loss:9.5276e-01,  Y0: 3.3174e+00,runtime: 188\n",
      "step:  900,loss:6.5362e-01,  Y0: 3.6606e+00,runtime: 206\n",
      "step: 1000,loss:3.7125e-01,  Y0: 4.0049e+00,runtime: 223\n",
      "step: 1100,loss:1.5589e-01,  Y0: 4.2818e+00,runtime: 240\n",
      "step: 1200,loss:4.8798e-02,  Y0: 4.4720e+00,runtime: 258\n",
      "step: 1300,loss:2.3950e-02,  Y0: 4.5604e+00,runtime: 276\n",
      "step: 1400,loss:2.1424e-02,  Y0: 4.5901e+00,runtime: 293\n",
      "step: 1500,loss:2.1841e-02,  Y0: 4.5950e+00,runtime: 310\n",
      "step: 1600,loss:2.1366e-02,  Y0: 4.5993e+00,runtime: 328\n",
      "step: 1700,loss:2.1008e-02,  Y0: 4.5985e+00,runtime: 347\n",
      "step: 1800,loss:2.1743e-02,  Y0: 4.6003e+00,runtime: 365\n",
      "step: 1900,loss:2.1306e-02,  Y0: 4.5980e+00,runtime: 382\n",
      "step: 2000,loss:2.1222e-02,  Y0: 4.5969e+00,runtime: 401\n",
      "running time:401.335 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , 17.80340145,  0.        ],\n",
       "       [ 0.        ,  4.92311993,  0.        ],\n",
       "       [ 0.        ,  2.46485239,  0.        ],\n",
       "       [ 0.        ,  2.26185662,  0.        ],\n",
       "       [ 0.        ,  2.04945293,  0.        ],\n",
       "       [ 0.        ,  1.85781516,  0.        ],\n",
       "       [ 0.        ,  1.56347269,  0.        ],\n",
       "       [ 0.        ,  1.23687674,  0.        ],\n",
       "       [ 0.        ,  0.95275863,  0.        ],\n",
       "       [ 0.        ,  0.65361974,  0.        ],\n",
       "       [ 0.        ,  0.37125142,  0.        ],\n",
       "       [ 0.        ,  0.15588514,  0.        ],\n",
       "       [ 0.        ,  0.04879787,  0.        ],\n",
       "       [ 0.        ,  0.0239502 ,  0.        ],\n",
       "       [ 0.        ,  0.02142406,  0.        ],\n",
       "       [ 0.        ,  0.02184074,  0.        ],\n",
       "       [ 0.        ,  0.02136637,  0.        ],\n",
       "       [ 0.        ,  0.02100846,  0.        ],\n",
       "       [ 0.        ,  0.02174252,  0.        ],\n",
       "       [ 0.        ,  0.02130574,  0.        ],\n",
       "       [ 0.        ,  0.02122176,  0.        ]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hjb_training(\"./HJB_d100.csv\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous traçons la fonction de perte avec 200 époques de 10 itérations. On remarque que l'algorithme diminue rapidement la perte avant d'atteindre un palier symbole de stagnation de l'apprentissage. Nous traçons également le graphique basé sur 20 époques de 100 itérations. \n",
    "\n",
    "Nous observons qu'avec un nombre d'itérations moins important dans le minibatch, la perte diminue plus vite. \n",
    "Par contre le palier de stagnation est atteint à peu près au même moment dans les deux graphiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x3d8aaac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "loss_history = pd.read_csv(\"HJB_d10.csv\")\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(121)\n",
    "plt.plot(loss_history.iloc[1:,1].values)\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "loss_history2 = pd.read_csv(\"HJB_d100.csv\")\n",
    "plt.subplot(122)\n",
    "plt.plot(loss_history2.iloc[1:,1].values)\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

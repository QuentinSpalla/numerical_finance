{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def essai(x):\n",
    "    return x+1\n",
    "\n",
    "\n",
    "res=essai(1)\n",
    "\n",
    "print(res)\n",
    "\n",
    "def bis(y):\n",
    "    return y+2\n",
    "res=bis(1)\n",
    "print(res) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin to solve Allen -Cahn\n",
      "WARNING:tensorflow:From <ipython-input-2-e13970af0bb8>:78: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "step:     0, loss: 5.5935e-02, Y0: 3.5068e-01,runtime:   31\n",
      "step:  100,loss:3.8714e-02,  Y0: 3.0230e-01,runtime:  58\n",
      "step:  200,loss:2.6160e-02,  Y0: 2.5911e-01,runtime:  73\n",
      "step:  300,loss:1.7261e-02,  Y0: 2.2066e-01,runtime:  88\n",
      "step:  400,loss:1.1083e-02,  Y0: 1.8716e-01,runtime: 102\n",
      "step:  500,loss:6.9629e-03,  Y0: 1.5833e-01,runtime: 116\n",
      "step:  600,loss:4.1624e-03,  Y0: 1.3388e-01,runtime: 131\n",
      "step:  700,loss:2.4662e-03,  Y0: 1.1369e-01,runtime: 146\n",
      "step:  800,loss:1.4387e-03,  Y0: 9.7355e-02,runtime: 161\n",
      "step:  900,loss:8.6324e-04,  Y0: 8.4553e-02,runtime: 175\n",
      "step: 1000,loss:5.5017e-04,  Y0: 7.4734e-02,runtime: 189\n",
      "step: 1100,loss:3.9184e-04,  Y0: 6.7450e-02,runtime: 204\n",
      "step: 1200,loss:3.1778e-04,  Y0: 6.2431e-02,runtime: 218\n",
      "step: 1300,loss:2.8410e-04,  Y0: 5.8817e-02,runtime: 232\n",
      "step: 1400,loss:2.6393e-04,  Y0: 5.6359e-02,runtime: 247\n",
      "step: 1500,loss:2.5306e-04,  Y0: 5.5006e-02,runtime: 263\n",
      "step: 1600,loss:2.5135e-04,  Y0: 5.4003e-02,runtime: 280\n",
      "step: 1700,loss:2.4813e-04,  Y0: 5.3537e-02,runtime: 296\n",
      "step: 1800,loss:2.4724e-04,  Y0: 5.3228e-02,runtime: 313\n",
      "step: 1900,loss:2.4271e-04,  Y0: 5.3073e-02,runtime: 329\n",
      "step: 2000,loss:2.3999e-04,  Y0: 5.2876e-02,runtime: 346\n",
      "step: 2100,loss:2.3991e-04,  Y0: 5.2987e-02,runtime: 363\n",
      "step: 2200,loss:2.4016e-04,  Y0: 5.2750e-02,runtime: 378\n",
      "step: 2300,loss:2.3363e-04,  Y0: 5.2960e-02,runtime: 394\n",
      "step: 2400,loss:2.3023e-04,  Y0: 5.2855e-02,runtime: 408\n",
      "step: 2500,loss:2.2654e-04,  Y0: 5.2749e-02,runtime: 423\n",
      "step: 2600,loss:2.2091e-04,  Y0: 5.2457e-02,runtime: 438\n",
      "step: 2700,loss:2.1667e-04,  Y0: 5.2737e-02,runtime: 452\n",
      "step: 2800,loss:2.0911e-04,  Y0: 5.2857e-02,runtime: 465\n",
      "step: 2900,loss:2.1039e-04,  Y0: 5.2808e-02,runtime: 479\n",
      "step: 3000,loss:2.0555e-04,  Y0: 5.3259e-02,runtime: 494\n",
      "step: 3100,loss:2.0044e-04,  Y0: 5.3023e-02,runtime: 509\n",
      "step: 3200,loss:1.9641e-04,  Y0: 5.2998e-02,runtime: 524\n",
      "step: 3300,loss:1.9011e-04,  Y0: 5.2689e-02,runtime: 539\n",
      "step: 3400,loss:1.8510e-04,  Y0: 5.2783e-02,runtime: 555\n",
      "step: 3500,loss:1.8329e-04,  Y0: 5.2787e-02,runtime: 571\n",
      "step: 3600,loss:1.7676e-04,  Y0: 5.2794e-02,runtime: 589\n",
      "step: 3700,loss:1.6686e-04,  Y0: 5.3207e-02,runtime: 605\n",
      "step: 3800,loss:1.6633e-04,  Y0: 5.2771e-02,runtime: 622\n",
      "step: 3900,loss:1.6148e-04,  Y0: 5.2818e-02,runtime: 639\n",
      "step: 4000,loss:1.5878e-04,  Y0: 5.2852e-02,runtime: 655\n",
      "running time:656.288 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.python.training.moving_averages import assign_moving_average\n",
    "from scipy.stats import multivariate_normal as normal\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from tensorflow import random_normal_initializer as norm_init\n",
    "from tensorflow import random_uniform_initializer as unif_init\n",
    "from tensorflow import constant_initializer as const_init\n",
    "\n",
    "class SolveAllenCahn(object):\n",
    "    def __init__(self, sess):\n",
    "     self.sess = sess\n",
    "     self.d = 100\n",
    "     self.T = 0.3\n",
    "     self.n_time = 20\n",
    "     self.n_layer =4\n",
    "     self.n_neuron = [self.d, self.d+10, self.d+10, self.d]\n",
    "     self.batch_size = 64\n",
    "     self.valid_size = 256\n",
    "     self.n_maxstep = 4000\n",
    "     self.n_displaystep = 100\n",
    "     self.learning_rate = 5e-4\n",
    "     self.Yini = [0.3, 0.6]\n",
    "     self.h = (self.T+0.0)/self.n_time\n",
    "     self.sqrth = math.sqrt(self.h)\n",
    "     self.t_stamp = np.arange(0, self.n_time)*self.h\n",
    "     self._extra_train_ops = []\n",
    "     \n",
    "    def train(self):\n",
    "     start_time = time.time()\n",
    "     self.global_step = tf.get_variable('global_step', [],initializer=tf.constant_initializer(1),trainable=False , dtype=tf.int32)\n",
    "     trainable_vars = tf.trainable_variables()\n",
    "     grads = tf.gradients(self.loss, trainable_vars)\n",
    "     optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "     apply_op = optimizer.apply_gradients(zip(grads, trainable_vars),global_step=self.global_step)\n",
    "     train_ops = [apply_op] + self._extra_train_ops\n",
    "     self.train_op = tf.group(*train_ops)\n",
    "     self.loss_history =[]\n",
    "     self.init_history =[]\n",
    "     dW_valid , X_valid= self.sample_path(self.valid_size) \n",
    "     feed_dict_valid ={self.dW: dW_valid , self.X: X_valid , self.is_training: False}\n",
    "     step=1\n",
    "     self.sess.run(tf.global_variables_initializer())\n",
    "     temp_loss = self.sess.run(self.loss, feed_dict=feed_dict_valid)\n",
    "     temp_init = self.Y0.eval()[0]\n",
    "     self.loss_history.append(temp_loss)\n",
    "     self.init_history.append(temp_init)\n",
    "     print(\"step: %5u, loss: %.4e, \"%(0, temp_loss)+\"Y0: %.4e,runtime: %4u\"%(temp_init,time.time()-start_time+self.t_bd))\n",
    "     for i in range(self.n_maxstep+1):\n",
    "         step = self.sess.run(self.global_step)\n",
    "         dW_train , X_train = self.sample_path(self.batch_size)\n",
    "         self.sess.run(self.train_op,feed_dict={self.dW: dW_train, self.X: X_train,self.is_training: True})\n",
    "         if  step % self.n_displaystep==0:\n",
    "             temp_loss = self.sess.run(self.loss,feed_dict=feed_dict_valid)\n",
    "             temp_init = self.Y0.eval()[0]\n",
    "             self.loss_history.append(temp_loss)\n",
    "             self.init_history.append(temp_init)\n",
    "             print(\"step:%5u,loss:%.4e, \" %(step,temp_loss), \"Y0:% .4e,runtime:% 4u\" %(temp_init,time.time()-start_time+self.t_bd))\n",
    "         step += 1\n",
    "     end_time = time.time()\n",
    "     print(\"running time:%.3f s\"%(end_time -start_time+self.t_bd))\n",
    "             \n",
    "    def build(self):\n",
    "                 start_time = time.time()\n",
    "                 self.dW =tf.placeholder(tf.float64 ,[None, self.d, self.n_time], name='dW')\n",
    "                 self.X = tf.placeholder(tf.float64 ,[None, self.d, self.n_time+1],name='X')\n",
    "                 self.is_training = tf.placeholder(tf.bool)\n",
    "                 self.Y0 = tf.Variable(tf.random_uniform([1],minval=self.Yini[0],maxval=self.Yini[1],dtype=tf.float64));\n",
    "                 self.Z0 = tf.Variable(tf.random_uniform([1, self.d],minval=-.1,maxval=.1,dtype=tf.float64))\n",
    "                 self.allones = tf.ones(shape=[tf.shape(self.dW)[0], 1],dtype=tf.float64)\n",
    "                 Y= self.allones * self.Y0\n",
    "                 Z= tf.matmul(self.allones , self.Z0)\n",
    "                 with tf.variable_scope('forward'):\n",
    "                     for t in range(0,self.n_time-1):\n",
    "                         Y= Y - self.f_tf(self.t_stamp[t],self.X[:, :, t], Y, Z)*self.h\n",
    "                         Y= Y + tf.reduce_sum(Z*self.dW[:, :, t], 1,keep_dims=True)\n",
    "                         Z = self._one_time_net(self.X[:, :, t+1],str(t+1))/self.d\n",
    "                     Y = Y - self.f_tf(self.t_stamp[self.n_time -1],self.X[:, :, self.n_time -1],Y, Z)*self.h\n",
    "                     Y = Y + tf.reduce_sum(Z*self.dW[:, :, self.n_time-1], 1,keep_dims=True)\n",
    "                     term_delta = Y - self.g_tf(self.T,self.X[:, :, self.n_time])\n",
    "                     self.clipped_delta = tf.clip_by_value(term_delta , -50.0, 50.0)\n",
    "                     self.loss = tf.reduce_mean(self.clipped_delta**2)\n",
    "                     self.t_bd = time.time()-start_time\n",
    "                \n",
    "    def sample_path(self, n_sample):\n",
    "        dW_sample = np.zeros([n_sample , self.d, self.n_time])\n",
    "        X_sample = np.zeros([n_sample, self.d, self.n_time+1])\n",
    "        for i in range(self.n_time):\n",
    "            dW_sample[:, :, i] = np.reshape(normal.rvs(mean=np.zeros(self.d),cov=1,size=n_sample)*self.sqrth,(n_sample, self.d))\n",
    "            X_sample[:, :, i+1] = X_sample[:, :, i] + np.sqrt(2) * dW_sample[:, :, i]\n",
    "        return dW_sample , X_sample\n",
    "        \n",
    "    def f_tf(self , t, X,  Y, Z):\n",
    "        return Y-tf.pow(Y, 3)\n",
    "    def g_tf(self, t, X):\n",
    "       return 0.5/(1 + 0.2*tf.reduce_sum(X**2, 1, keep_dims=True))\n",
    "   \n",
    "    \n",
    "    def _one_time_net(self, x, name):\n",
    "       with tf.variable_scope(name):\n",
    "           x_norm = self._batch_norm(x, name='layer0_normal')\n",
    "           layer1 = self._one_layer(x_norm, self.n_neuron[1],name='layer1')\n",
    "           layer2 = self._one_layer(layer1, self.n_neuron[2],name='layer2')\n",
    "           z = self._one_layer(layer2, self.n_neuron[3], activation_fn=None, name='final')\n",
    "       return z\n",
    "       \n",
    "        \n",
    "    def _one_layer(self, input_, out_sz,activation_fn=tf.nn.relu,std=5.0, name='linear'):\n",
    "            with tf.variable_scope(name):\n",
    "                shape = input_.get_shape().as_list()\n",
    "                w = tf.get_variable('Matrix',[shape[1], out_sz],tf.float64 , norm_init(stddev= std/np.sqrt(shape[1]+out_sz))) \n",
    "                hidden=tf.matmul(input_ , w)\n",
    "                hidden_bn = self._batch_norm(hidden, name='normal')\n",
    "            if activation_fn != None:\n",
    "                    return activation_fn(hidden_bn)\n",
    "            else:\n",
    "                    return hidden_bn\n",
    "                \n",
    "    def _batch_norm(self, x, name):\n",
    "            with tf.variable_scope(name):\n",
    "                params_shape = [x.get_shape()[-1]]\n",
    "                beta = tf.get_variable('beta', params_shape , tf.float64 ,norm_init(0.0, stddev=0.1,dtype=tf.float64))\n",
    "                gamma =tf.get_variable('gamma', params_shape , tf.float64 ,unif_init(0.1, 0.5, dtype=tf.float64))\n",
    "                mv_mean = tf.get_variable('moving_mean', params_shape,tf.float64,const_init(0.0, tf.float64), trainable=False)\n",
    "                mv_var = tf.get_variable('moving_variance', params_shape,tf.float64,const_init(1.0, tf.float64), trainable=False)\n",
    "                mean, variance = tf.nn.moments(x, [0], name='moments')\n",
    "                self._extra_train_ops.append(assign_moving_average(mv_mean, mean, 0.99))\n",
    "                self._extra_train_ops.append(assign_moving_average(mv_var , variance , 0.99))\n",
    "                mean , variance = control_flow_ops.cond(self.is_training ,lambda: (mean, variance),lambda: (mv_mean , mv_var))\n",
    "                y = tf.nn.batch_normalization(x, mean, variance, beta, gamma, 1e-6)\n",
    "                y.set_shape(x.get_shape())\n",
    "                return y\n",
    "        \n",
    "def main():\n",
    "       tf.reset_default_graph()\n",
    "       with tf.Session() as sess:\n",
    "           tf.set_random_seed(1)\n",
    "           print (\"Begin to solve Allen -Cahn\")\n",
    "           model = SolveAllenCahn(sess)\n",
    "           model.build()\n",
    "           model.train()\n",
    "           output = np.zeros((len(model.init_history), 3))\n",
    "           output[:, 0] = np.arange(len(model.init_history))* model.n_displaystep\n",
    "           output[:, 1] = model.loss_history\n",
    "           output[:, 2] = model.init_history\n",
    "           np.savetxt(\"./AllenCahn_d100.csv\", output ,fmt=['%d', '%.5e', '%.5e'], delimiter=\",\",header=\"step, loss function, \" +  \"target value ,runtime\", comments='')\n",
    "if __name__ == '__main__':\n",
    "               np.random.seed(1)\n",
    "               main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
